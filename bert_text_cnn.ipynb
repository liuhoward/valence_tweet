{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_text_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kjY1nWCy15CA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "reference:  \n",
        "[google bert](https://github.com/google-research/bert)  \n",
        "[keras-bert-tpu](https://colab.research.google.com/github/HighCWu/keras-bert-tpu/blob/master/demo/load_model/load_and_predict.ipynb#scrollTo=GBFMypMHSHlt)  \n",
        "[keras-bert](https://github.com/CyberZHG/keras-bert)  \n",
        "[keras-bert jupyter notebook](https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb)\n"
      ]
    },
    {
      "metadata": {
        "id": "UaSHnWP4xP6h",
        "colab_type": "code",
        "outputId": "c57c7258-86b7-4508-88fa-70211267f723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h9x4Tjcc0dDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "a99ebed8-9282-4837-94b1-7d36897418ad"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "UPLOAD_TIME = '2018_10_18' #@param {type:\"string\"}\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "download_url = 'https://storage.googleapis.com/bert_models/{}/{}.zip'.format(UPLOAD_TIME,BERT_MODEL)\n",
        "zip_path = '{}.zip'.format(BERT_MODEL)\n",
        "! test -d $BERT_MODEL || (wget $download_url && unzip $zip_path)\n",
        "BERT_PRETRAINED_DIR = os.path.realpath(BERT_MODEL)\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-28 03:13:08--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128, 2607:f8b0:4001:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   119MB/s    in 3.3s    \n",
            "\n",
            "2019-04-28 03:13:11 (119 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
            "***** BERT pretrained directory: /content/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3MlCFrG2f_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a69efc60-0b92-464a-ba44-0af180f8cdac"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow==1.13.1\n",
        "!pip install -q bert-tensorflow\n",
        "!pip install -q keras-bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    15% |████▉                           | 10kB 26.2MB/s eta 0:00:01\r\u001b[K    30% |█████████▊                      | 20kB 2.2MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 30kB 3.2MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▍            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 51kB 2.6MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 61kB 3.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25h  Building wheel for keras-bert (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XfwkD_XU9_FW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import layers, models, optimizers, initializers\n",
        "from keras import regularizers\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import json\n",
        "from bert.tokenization import FullTokenizer\n",
        "from keras_bert.loader import load_trained_model_from_checkpoint\n",
        "from keras_bert.bert import *\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eahHvBABuv6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/My Drive/\"\n",
        "#train_tweet_npz = data_path + \"train_tweets.npz\"\n",
        "#train_tweet = np.load(train_tweet_npz)\n",
        "#test_tweet_npz = data_path + \"test_tweets.npz\"\n",
        "#test_tweet = np.load(test_tweet_npz)\n",
        "#dev_tweet_npz = data_path + \"dev_tweets.npz\"\n",
        "#dev_tweet = np.load(dev_tweet_npz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7u49lL7MFzmn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_train(src_file: str):\n",
        "    \"\"\"Generates (id, tweet, dimension, score) tuples from the lines in an src file.\n",
        "\n",
        "    :param src_file:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    id_list = list()\n",
        "    tweet_list = list()\n",
        "    # dimension_list = list()\n",
        "    intensity_list = list()\n",
        "    with open(src_file) as fp:\n",
        "        reader = csv.DictReader(fp, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            record_id = row['ID']\n",
        "            tweet = row['Tweet']\n",
        "            dimension = row['Affect Dimension']\n",
        "            if dimension != 'valence':\n",
        "                print(record_id)\n",
        "            intensity = (row['Intensity Class']).split(': ')[0]\n",
        "            id_list.append(record_id)\n",
        "            tweet_list.append(tweet)\n",
        "            # dimension_list.append(dimension)\n",
        "            intensity_list.append(intensity)\n",
        "\n",
        "    return id_list, tweet_list, intensity_list\n",
        "\n",
        "\n",
        "def read_test(src_file: str):\n",
        "    \"\"\"Generates (id, tweet, dimension, score) tuples from the lines in an src file.\n",
        "\n",
        "    :param src_file:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    id_list = list()\n",
        "    dimension_list = list()\n",
        "    tweet_list = list()\n",
        "    with open(src_file) as fp:\n",
        "        reader = csv.DictReader(fp, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            record_id = row['ID']\n",
        "            tweet = row['Tweet']\n",
        "            dimension = row['Affect Dimension']\n",
        "            if dimension != 'valence':\n",
        "                print(record_id)\n",
        "            id_list.append(record_id)\n",
        "            dimension_list.append(dimension)\n",
        "            tweet_list.append(tweet)\n",
        "\n",
        "    return id_list, tweet_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqaN2te4E_vR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_file = data_path + '2018-Valence-oc-En-train.txt'\n",
        "test_file = data_path + '2018-Valence-oc-En-test.txt'\n",
        "dev_file = data_path + '2018-Valence-oc-En-dev.txt'\n",
        "\n",
        "vocab_file = data_path + 'vocab_emotion.txt'\n",
        "\n",
        "train_id, train_tweet, intensity_list = read_train(train_file)\n",
        "train_labels = [int(x) + 3 for x in intensity_list]\n",
        "\n",
        "dev_id, dev_tweet, intensity_list = read_train(dev_file)\n",
        "dev_labels = [int(x) + 3 for x in intensity_list]\n",
        "\n",
        "test_id, test_tweet = read_test(test_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HG9tdLwPwQDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels = np.reshape(np.vstack(train_labels), newshape=(-1))\n",
        "dev_labels = np.reshape(np.vstack(dev_labels), newshape=(-1))\n",
        "test_labels = np.zeros(len(test_id), dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2qDsseIDLK7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels).reshape(-1, 1),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0FOjhbvNu11",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence_length = 128\n",
        "\n",
        "# Instantiate tokenizer\n",
        "tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
        "\n",
        "# Convert data to InputExample format\n",
        "train_examples = convert_text_to_examples(train_tweet, train_labels)\n",
        "dev_examples = convert_text_to_examples(dev_tweet, dev_labels)\n",
        "\n",
        "test_examples = convert_text_to_examples(test_tweet, test_labels)\n",
        "\n",
        "# Convert to features\n",
        "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=sequence_length)\n",
        "(dev_input_ids, dev_input_masks, dev_segment_ids, dev_labels\n",
        ") = convert_examples_to_features(tokenizer, dev_examples, max_seq_length=sequence_length)\n",
        "\n",
        "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
        ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=sequence_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G2DecKxd_M90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2mjjDjv13Pj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "bert_model = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True, seq_len=sequence_length)\n",
        "\n",
        "# bert_model.summary(line_length=120)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYbkoov_eNDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bert_output = bert_model.get_layer(name='Encoder-12-FeedForward-Norm').output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkIfyCnQS6f0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 768\n",
        "batch_size = 32\n",
        "drop = 0.9\n",
        "\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWCN4en5UF7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4165
        },
        "outputId": "5e59b944-b18b-416c-8242-a42bfcc248fd"
      },
      "cell_type": "code",
      "source": [
        "############################\n",
        "# use output of first token (CLS) for classification\n",
        "\n",
        "squeezed = layers.Lambda(lambda x: K.squeeze(x[:, 0:1, :], axis=1))(bert_output)\n",
        "#reshape_data = layers.Reshape((sequence_length, embedding_dim, 1))(bert_output)\n",
        "\n",
        "\n",
        "#dense1 = layers.Dense(units=128, activation='relu',\n",
        "#                        kernel_initializer='glorot_uniform', \n",
        "#                        kernel_regularizer=regularizers.l2(0.06))(squeezed)\n",
        "\n",
        "#dropout = layers.Dropout(drop)(dense1)\n",
        "output = layers.Dense(units=7, activation='softmax', \n",
        "                      kernel_initializer=initializers.truncated_normal(stddev=0.02)\n",
        "                      )(squeezed)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = models.Model(inputs=bert_model.inputs, outputs=output)\n",
        "\n",
        "opt = optimizers.Adam(lr=1e-4)##, decay=1e-6)\n",
        "#opt = optimizers.SGD(lr=1e-4)\n",
        "\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 128, 768), ( 23440896    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 128, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 128, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 128, 768)     98304       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 128, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 128, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 128, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 128, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 128, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_39 (Dense)                (None, 7)            5383        lambda_18[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 108,602,119\n",
            "Trainable params: 108,602,119\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_QvMVaR5DdDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "42d92609-cb7b-43be-b39a-039da99d7bad"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [train_input_ids, train_input_masks, train_segment_ids], \n",
        "    train_labels,\n",
        "    validation_data=([dev_input_ids, dev_input_masks, dev_segment_ids], dev_labels),\n",
        "    epochs=100,\n",
        "    batch_size=32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1181 samples, validate on 449 samples\n",
            "Epoch 1/100\n",
            " 192/1181 [===>..........................] - ETA: 2:27 - loss: 1.8667 - acc: 0.2760"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QWlYeDhKctQh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save the trials object\n",
        "with open(save_file, \"wb\") as f:\n",
        "    pickle.dump(history.history, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rFrVRmfK5oaC",
        "colab_type": "code",
        "outputId": "1d181e2e-6552-43d1-ed9d-029928463c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4641
        }
      },
      "cell_type": "code",
      "source": [
        "filter_sizes = [2,3,4]\n",
        "num_filters = 256\n",
        "\n",
        "#############################\n",
        "## build text cnn model\n",
        "#############################\n",
        "\n",
        "nomask = layers.Lambda(lambda x: x, output_shape=lambda s:s)(bert_output)\n",
        "reshape_data = layers.Reshape((sequence_length, embedding_dim, 1))(nomask)\n",
        "\n",
        "pooled_outputs = []\n",
        "for i, filter_size in enumerate(filter_sizes):\n",
        "    conv_layer = layers.Conv2D(num_filters, \n",
        "                               kernel_size=(filter_sizes[i], embedding_dim), \n",
        "                               padding='valid', \n",
        "                               kernel_initializer='glorot_uniform', \n",
        "                               kernel_regularizer=regularizers.l2(0.1), activation='relu')(reshape_data)\n",
        "    bn_layer = layers.BatchNormalization()(conv_layer)\n",
        "    pool_layer = layers.MaxPool2D(pool_size=(sequence_length - filter_sizes[i] + 1, 1), strides=(1,1), padding='valid')(bn_layer)\n",
        "    pooled_outputs.append(pool_layer)\n",
        "\n",
        "concatenated_tensor = layers.Concatenate(axis=1)(pooled_outputs)\n",
        "flatten = layers.Flatten()(concatenated_tensor)\n",
        "#flatten1 = layers.Dense(units=128, activation='relu',\n",
        "#                        kernel_initializer=initializers.truncated_normal(stddev=0.02), \n",
        "#                        kernel_regularizer=regularizers.l2(0.1))(flatten)\n",
        "\n",
        "dropout = layers.Dropout(drop)(flatten)\n",
        "output = layers.Dense(units=7, activation='softmax', \n",
        "                      kernel_initializer=initializers.truncated_normal(stddev=0.02), \n",
        "                      kernel_regularizer=regularizers.l2(0.1))(dropout)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = models.Model(inputs=bert_model.inputs, outputs=output)\n",
        "\n",
        "opt = optimizers.Adam(lr=1e-5, decay=1e-6)\n",
        "#opt = optimizers.SGD(lr=1e-4)\n",
        "\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 128, 768), ( 23440896    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 128, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 128, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 128, 768)     98304       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 128, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 128, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 128, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 128, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 128, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 128, 768)     0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 128, 768, 1)  0           lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 127, 1, 256)  393472      reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 126, 1, 256)  590080      reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 125, 1, 256)  786688      reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 127, 1, 256)  1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 126, 1, 256)  1024        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 125, 1, 256)  1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 256)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 256)    0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 1, 1, 256)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3, 1, 256)    0           max_pooling2d_7[0][0]            \n",
            "                                                                 max_pooling2d_8[0][0]            \n",
            "                                                                 max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 768)          0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 768)          0           flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 7)            5383        dropout_17[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 110,375,431\n",
            "Trainable params: 110,373,895\n",
            "Non-trainable params: 1,536\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HDA3RHt-seTl",
        "colab_type": "code",
        "outputId": "47e6e64d-0be4-44f7-d443-c99063d24646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"train model ==============================\")\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_dev, y_dev))  # starts training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train model ==============================\n",
            "Train on 1181 samples, validate on 449 samples\n",
            "Epoch 1/100\n",
            "1181/1181 [==============================] - 5s 4ms/sample - loss: 20.0445 - acc: 0.1702 - val_loss: 16.4608 - val_acc: 0.2249\n",
            "Epoch 2/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 16.9835 - acc: 0.2227 - val_loss: 16.4380 - val_acc: 0.2160\n",
            "Epoch 3/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 16.4276 - acc: 0.2371 - val_loss: 16.4046 - val_acc: 0.2227\n",
            "Epoch 4/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 16.2265 - acc: 0.3158 - val_loss: 16.3665 - val_acc: 0.2405\n",
            "Epoch 5/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 16.1231 - acc: 0.3311 - val_loss: 16.3278 - val_acc: 0.2517\n",
            "Epoch 6/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 16.0404 - acc: 0.3412 - val_loss: 16.2887 - val_acc: 0.2539\n",
            "Epoch 7/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.9271 - acc: 0.3726 - val_loss: 16.2474 - val_acc: 0.2784\n",
            "Epoch 8/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.8296 - acc: 0.3853 - val_loss: 16.2062 - val_acc: 0.2806\n",
            "Epoch 9/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.6955 - acc: 0.4386 - val_loss: 16.1625 - val_acc: 0.2717\n",
            "Epoch 10/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.6651 - acc: 0.4259 - val_loss: 16.1209 - val_acc: 0.2762\n",
            "Epoch 11/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.5245 - acc: 0.4818 - val_loss: 16.0778 - val_acc: 0.2829\n",
            "Epoch 12/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.4790 - acc: 0.4742 - val_loss: 16.0328 - val_acc: 0.2940\n",
            "Epoch 13/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.3828 - acc: 0.5030 - val_loss: 15.9845 - val_acc: 0.2873\n",
            "Epoch 14/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.3024 - acc: 0.5334 - val_loss: 15.9421 - val_acc: 0.2873\n",
            "Epoch 15/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.1990 - acc: 0.5318 - val_loss: 15.8889 - val_acc: 0.3051\n",
            "Epoch 16/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.1096 - acc: 0.5572 - val_loss: 15.8420 - val_acc: 0.2984\n",
            "Epoch 17/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 15.0457 - acc: 0.5546 - val_loss: 15.7988 - val_acc: 0.2984\n",
            "Epoch 18/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.9894 - acc: 0.5851 - val_loss: 15.7437 - val_acc: 0.3007\n",
            "Epoch 19/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.9422 - acc: 0.5648 - val_loss: 15.6847 - val_acc: 0.3029\n",
            "Epoch 20/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.8380 - acc: 0.5970 - val_loss: 15.6358 - val_acc: 0.3140\n",
            "Epoch 21/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.7521 - acc: 0.6249 - val_loss: 15.5869 - val_acc: 0.3029\n",
            "Epoch 22/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.7112 - acc: 0.6266 - val_loss: 15.5334 - val_acc: 0.2984\n",
            "Epoch 23/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.6428 - acc: 0.6317 - val_loss: 15.4910 - val_acc: 0.2918\n",
            "Epoch 24/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.5778 - acc: 0.6571 - val_loss: 15.4213 - val_acc: 0.3274\n",
            "Epoch 25/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.5101 - acc: 0.6596 - val_loss: 15.3734 - val_acc: 0.3163\n",
            "Epoch 26/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.4285 - acc: 0.6630 - val_loss: 15.3148 - val_acc: 0.3229\n",
            "Epoch 27/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.3844 - acc: 0.6638 - val_loss: 15.2731 - val_acc: 0.3096\n",
            "Epoch 28/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.2825 - acc: 0.7036 - val_loss: 15.2042 - val_acc: 0.3163\n",
            "Epoch 29/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.2618 - acc: 0.6816 - val_loss: 15.1487 - val_acc: 0.3385\n",
            "Epoch 30/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.2181 - acc: 0.6867 - val_loss: 15.0918 - val_acc: 0.3185\n",
            "Epoch 31/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.1141 - acc: 0.7214 - val_loss: 15.0436 - val_acc: 0.3452\n",
            "Epoch 32/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.0612 - acc: 0.7316 - val_loss: 14.9902 - val_acc: 0.3385\n",
            "Epoch 33/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 14.0301 - acc: 0.7045 - val_loss: 14.9544 - val_acc: 0.3363\n",
            "Epoch 34/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.9755 - acc: 0.7350 - val_loss: 14.9064 - val_acc: 0.3519\n",
            "Epoch 35/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.8933 - acc: 0.7426 - val_loss: 14.8498 - val_acc: 0.3474\n",
            "Epoch 36/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.8503 - acc: 0.7384 - val_loss: 14.8172 - val_acc: 0.3363\n",
            "Epoch 37/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.7969 - acc: 0.7426 - val_loss: 14.7739 - val_acc: 0.3274\n",
            "Epoch 38/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.7508 - acc: 0.7595 - val_loss: 14.7256 - val_acc: 0.3474\n",
            "Epoch 39/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.6811 - acc: 0.7536 - val_loss: 14.6763 - val_acc: 0.3586\n",
            "Epoch 40/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.6365 - acc: 0.7629 - val_loss: 14.6545 - val_acc: 0.3474\n",
            "Epoch 41/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.5653 - acc: 0.7731 - val_loss: 14.6282 - val_acc: 0.3430\n",
            "Epoch 42/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.5242 - acc: 0.7739 - val_loss: 14.5759 - val_acc: 0.3519\n",
            "Epoch 43/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.4902 - acc: 0.7680 - val_loss: 14.5577 - val_acc: 0.3630\n",
            "Epoch 44/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.4481 - acc: 0.7663 - val_loss: 14.4908 - val_acc: 0.3474\n",
            "Epoch 45/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.3844 - acc: 0.7790 - val_loss: 14.4726 - val_acc: 0.3318\n",
            "Epoch 46/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.3068 - acc: 0.7951 - val_loss: 14.4335 - val_acc: 0.3430\n",
            "Epoch 47/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.2517 - acc: 0.8002 - val_loss: 14.3942 - val_acc: 0.3408\n",
            "Epoch 48/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.2284 - acc: 0.7934 - val_loss: 14.3631 - val_acc: 0.3296\n",
            "Epoch 49/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.1211 - acc: 0.8171 - val_loss: 14.3170 - val_acc: 0.3519\n",
            "Epoch 50/100\n",
            "1181/1181 [==============================] - 4s 4ms/sample - loss: 13.1182 - acc: 0.8036 - val_loss: 14.2988 - val_acc: 0.3563\n",
            "Epoch 51/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.0280 - acc: 0.8281 - val_loss: 14.2381 - val_acc: 0.3341\n",
            "Epoch 52/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 13.0534 - acc: 0.8095 - val_loss: 14.2171 - val_acc: 0.3497\n",
            "Epoch 53/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.9825 - acc: 0.8086 - val_loss: 14.1850 - val_acc: 0.3452\n",
            "Epoch 54/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.9496 - acc: 0.7959 - val_loss: 14.1130 - val_acc: 0.3452\n",
            "Epoch 55/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.8560 - acc: 0.8307 - val_loss: 14.0876 - val_acc: 0.3408\n",
            "Epoch 56/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.8176 - acc: 0.8230 - val_loss: 14.0480 - val_acc: 0.3608\n",
            "Epoch 57/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.8111 - acc: 0.8010 - val_loss: 14.0495 - val_acc: 0.3452\n",
            "Epoch 58/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.7380 - acc: 0.8315 - val_loss: 13.9675 - val_acc: 0.3653\n",
            "Epoch 59/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.6752 - acc: 0.8290 - val_loss: 13.9655 - val_acc: 0.3608\n",
            "Epoch 60/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.6457 - acc: 0.8290 - val_loss: 13.9210 - val_acc: 0.3608\n",
            "Epoch 61/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.6120 - acc: 0.8188 - val_loss: 13.8372 - val_acc: 0.3519\n",
            "Epoch 62/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.5520 - acc: 0.8349 - val_loss: 13.8264 - val_acc: 0.3630\n",
            "Epoch 63/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.5249 - acc: 0.8332 - val_loss: 13.7871 - val_acc: 0.3541\n",
            "Epoch 64/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.4838 - acc: 0.8222 - val_loss: 13.7142 - val_acc: 0.3563\n",
            "Epoch 65/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.3952 - acc: 0.8484 - val_loss: 13.7328 - val_acc: 0.3519\n",
            "Epoch 66/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.3517 - acc: 0.8518 - val_loss: 13.6704 - val_acc: 0.3563\n",
            "Epoch 67/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.2873 - acc: 0.8561 - val_loss: 13.6183 - val_acc: 0.3563\n",
            "Epoch 68/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.2430 - acc: 0.8637 - val_loss: 13.5908 - val_acc: 0.3541\n",
            "Epoch 69/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.2659 - acc: 0.8374 - val_loss: 13.5722 - val_acc: 0.3586\n",
            "Epoch 70/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.1756 - acc: 0.8671 - val_loss: 13.5166 - val_acc: 0.3563\n",
            "Epoch 71/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.1507 - acc: 0.8501 - val_loss: 13.5260 - val_acc: 0.3697\n",
            "Epoch 72/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.1111 - acc: 0.8476 - val_loss: 13.4461 - val_acc: 0.3474\n",
            "Epoch 73/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 12.0369 - acc: 0.8688 - val_loss: 13.3961 - val_acc: 0.3719\n",
            "Epoch 74/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.9985 - acc: 0.8620 - val_loss: 13.3577 - val_acc: 0.3653\n",
            "Epoch 75/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.9989 - acc: 0.8501 - val_loss: 13.3027 - val_acc: 0.3697\n",
            "Epoch 76/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.9495 - acc: 0.8535 - val_loss: 13.2859 - val_acc: 0.3586\n",
            "Epoch 77/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.8855 - acc: 0.8586 - val_loss: 13.2322 - val_acc: 0.3697\n",
            "Epoch 78/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.8595 - acc: 0.8535 - val_loss: 13.1770 - val_acc: 0.3586\n",
            "Epoch 79/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.7977 - acc: 0.8679 - val_loss: 13.1460 - val_acc: 0.3608\n",
            "Epoch 80/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.7511 - acc: 0.8620 - val_loss: 13.0997 - val_acc: 0.3563\n",
            "Epoch 81/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.7108 - acc: 0.8730 - val_loss: 13.1207 - val_acc: 0.3764\n",
            "Epoch 82/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.6499 - acc: 0.8772 - val_loss: 13.0559 - val_acc: 0.3742\n",
            "Epoch 83/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.6088 - acc: 0.8882 - val_loss: 13.0166 - val_acc: 0.3719\n",
            "Epoch 84/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.5607 - acc: 0.8730 - val_loss: 13.0120 - val_acc: 0.3786\n",
            "Epoch 85/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.5271 - acc: 0.8815 - val_loss: 12.9564 - val_acc: 0.3675\n",
            "Epoch 86/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.4885 - acc: 0.8738 - val_loss: 12.8986 - val_acc: 0.3563\n",
            "Epoch 87/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.4733 - acc: 0.8637 - val_loss: 12.8933 - val_acc: 0.3608\n",
            "Epoch 88/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.4079 - acc: 0.8865 - val_loss: 12.8328 - val_acc: 0.3719\n",
            "Epoch 89/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.3879 - acc: 0.8764 - val_loss: 12.7746 - val_acc: 0.3586\n",
            "Epoch 90/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.3131 - acc: 0.8916 - val_loss: 12.7651 - val_acc: 0.3630\n",
            "Epoch 91/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.3034 - acc: 0.8806 - val_loss: 12.6990 - val_acc: 0.3653\n",
            "Epoch 92/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.2585 - acc: 0.8713 - val_loss: 12.6652 - val_acc: 0.3608\n",
            "Epoch 93/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.2169 - acc: 0.8772 - val_loss: 12.6452 - val_acc: 0.3608\n",
            "Epoch 94/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.1632 - acc: 0.8848 - val_loss: 12.6061 - val_acc: 0.3697\n",
            "Epoch 95/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.1189 - acc: 0.8874 - val_loss: 12.5841 - val_acc: 0.3786\n",
            "Epoch 96/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.1040 - acc: 0.8721 - val_loss: 12.5032 - val_acc: 0.3742\n",
            "Epoch 97/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 11.0187 - acc: 0.9043 - val_loss: 12.4895 - val_acc: 0.3898\n",
            "Epoch 98/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 10.9891 - acc: 0.8942 - val_loss: 12.4251 - val_acc: 0.3719\n",
            "Epoch 99/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 10.9496 - acc: 0.8992 - val_loss: 12.4166 - val_acc: 0.3742\n",
            "Epoch 100/100\n",
            "1181/1181 [==============================] - 4s 3ms/sample - loss: 10.9117 - acc: 0.8984 - val_loss: 12.3808 - val_acc: 0.3742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2EXxc0minPVk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_results_file = data_path + 'test_results.txt'\n",
        "y_pred = model.predict(x=x_test)\n",
        "np.savetxt(test_results_file, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B0rdqM3TVDA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(800,800))\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FnLL-DCy_Ao8",
        "colab_type": "code",
        "outputId": "bbf3b2a3-acf2-429b-b7c8-3c0cac2b6f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(800,800))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 57600x57600 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 57600x57600 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XRBo8LX2IUnd",
        "colab_type": "code",
        "outputId": "42a687bd-c91b-4f4a-db70-f148d54d4a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5i3Fiy9tIwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AWzt0Yj1IXwq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}